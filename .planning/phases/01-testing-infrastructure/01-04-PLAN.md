---
phase: 01-testing-infrastructure
plan: 04
type: execute
wave: 3
depends_on: ["01-02", "01-03"]
files_modified:
  - tests/integration/commands/init.test.ts
  - tests/integration/commands/add.test.ts
  - tests/integration/commands/sync.test.ts
  - tests/integration/commands/status.test.ts
  - tests/integration/commands/clean.test.ts
  - tests/integration/commands/update.test.ts
autonomous: true

must_haves:
  truths:
    - "Integration tests exercise each CLI command (init, add, sync, status, clean, update) with all I/O mocked"
    - "No integration test hits real filesystem, network, or MCP subprocess"
    - "Each command test mocks prompts library for interactive paths"
    - "Running `bun test` executes all unit AND integration tests with all passing"
    - "Per-module coverage script confirms 80%+ coverage on each src/lib/ module"
    - "Coverage report generates lcov output for CI tooling"
  artifacts:
    - path: "tests/integration/commands/init.test.ts"
      provides: "Init command integration tests"
      min_lines: 60
    - path: "tests/integration/commands/add.test.ts"
      provides: "Add command integration tests"
      min_lines: 80
    - path: "tests/integration/commands/sync.test.ts"
      provides: "Sync command integration tests"
      min_lines: 60
    - path: "tests/integration/commands/status.test.ts"
      provides: "Status command integration tests"
      min_lines: 60
    - path: "tests/integration/commands/clean.test.ts"
      provides: "Clean command integration tests"
      min_lines: 60
    - path: "tests/integration/commands/update.test.ts"
      provides: "Update command integration tests"
      min_lines: 60
  key_links:
    - from: "tests/integration/commands/init.test.ts"
      to: "src/commands/init.ts"
      via: "dynamic import after mocking"
      pattern: "import.*commands/init"
    - from: "tests/integration/commands/add.test.ts"
      to: "src/commands/add.ts"
      via: "dynamic import after mocking"
      pattern: "import.*commands/add"
    - from: "tests/integration/commands/add.test.ts"
      to: "tests/helpers/mock-fetch.ts"
      via: "Context7 HTTP mock for doc fetching"
      pattern: "createFetchMock|spyOn.*fetch"
    - from: "tests/integration/commands/*.test.ts"
      to: "tests/helpers/mock-fs.ts"
      via: "filesystem mocking"
      pattern: "mock\\.module.*node:fs"
---

<objective>
Write integration tests for all 6 CLI commands and validate that the complete test suite achieves 80%+ coverage per module.

Purpose: Integration tests verify that commands orchestrate library modules correctly with all I/O mocked. The coverage validation confirms the phase goal is met: developers can trust the test suite before making changes.

Output: Six integration test files in tests/integration/commands/ and confirmed 80%+ per-module coverage.
</objective>

<execution_context>
@/Users/sleepy/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sleepy/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-testing-infrastructure/01-CONTEXT.md
@.planning/phases/01-testing-infrastructure/01-RESEARCH.md
@.planning/phases/01-testing-infrastructure/01-01-SUMMARY.md
@.planning/phases/01-testing-infrastructure/01-02-SUMMARY.md
@.planning/phases/01-testing-infrastructure/01-03-SUMMARY.md
@src/commands/init.ts
@src/commands/add.ts
@src/commands/sync.ts
@src/commands/status.ts
@src/commands/clean.ts
@src/commands/update.ts
@src/lib/types.ts
@tests/helpers/setup.ts
@tests/helpers/mock-fetch.ts
@tests/helpers/mock-fs.ts
@tests/helpers/mock-mcp.ts
@tests/helpers/factories.ts
@tests/fixtures/config/valid-config.json
@tests/fixtures/package-json/cli-project.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Integration tests for all 6 CLI commands</name>
  <files>
    tests/integration/commands/init.test.ts
    tests/integration/commands/add.test.ts
    tests/integration/commands/sync.test.ts
    tests/integration/commands/status.test.ts
    tests/integration/commands/clean.test.ts
    tests/integration/commands/update.test.ts
  </files>
  <action>
    Write integration tests for each CLI command. Each test file mocks ALL external I/O (filesystem, fetch, prompts, console output) and tests the command's orchestration logic.

    **Common pattern for ALL command tests:**
    1. Call mock.module("node:fs", ...) and mock.module("node:fs/promises", ...) at file top
    2. Call mock.module("prompts", ...) for interactive commands
    3. Mock ora (the spinner library) to prevent terminal output: mock.module("ora", () => ({ default: () => ({ start: () => spinner, succeed: () => {}, fail: () => {}, stop: () => {} }) }))
    4. Mock chalk to pass through strings without ANSI: mock.module("chalk", () => a proxy that returns the input string
    5. spyOn(console, "log") and spyOn(console, "error") to capture output
    6. Use dynamic import() for the command module AFTER all mocks are set up
    7. Pass projectRoot via options to avoid process.cwd() dependency (enabled by Plan 01 refactoring)
    8. Reset mocks in beforeEach

    **init.test.ts:**

    `describe("initCommand")`:
    - Creates .claude-docs structure and config.json when not initialized
    - Skips initialization when already initialized (without --force)
    - Re-initializes when --force is true
    - Detects project type from package.json
    - Detects framework dependencies and shows them in output
    - Handles missing package.json gracefully (shows error)
    - Updates .gitignore with cache entry

    **add.test.ts:**

    `describe("addCommand")`:
    - Adds a framework by name when template exists (mock Context7 HTTP fetch for doc content)
    - Creates doc files in correct directory structure
    - Updates config.json with framework entry
    - Updates CLAUDE.md index after adding
    - Handles unknown framework name (shows error)
    - Handles Context7 API failure (shows error, suggests offline mode)
    - Prompts for framework selection when called interactively (mock prompts)

    **sync.test.ts:**

    `describe("syncCommand")`:
    - Detects frameworks from package.json and adds missing ones
    - Skips already-indexed frameworks
    - Prompts for confirmation before syncing (mock prompts to return yes)
    - Does nothing when prompts returns no
    - Handles --yes flag (skips confirmation)

    **status.test.ts:**

    `describe("statusCommand")`:
    - Shows framework list with versions and file counts
    - Shows index size information
    - Shows last sync time
    - Handles project with no frameworks configured
    - Handles missing config.json (shows "not initialized" message)

    **clean.test.ts:**

    `describe("cleanCommand")`:
    - Removes specified framework docs and config entry
    - Prompts for confirmation before cleaning (mock prompts)
    - Updates CLAUDE.md index after removal
    - Handles non-existent framework name

    **update.test.ts:**

    `describe("updateCommand")`:
    - Re-fetches docs for specified frameworks
    - Updates all frameworks when none specified
    - Prompts for confirmation (mock prompts)
    - Handles Context7 API failure during update
    - Updates config.json with new lastUpdate timestamp

    **Important notes:**
    - Do NOT test every edge case of library functions (those are covered in unit tests). Focus on command ORCHESTRATION: does the command call the right library functions in the right order with the right arguments?
    - Use spyOn assertions to verify function calls rather than deeply inspecting mock filesystem state
    - Each test should be self-contained: set up mock state, run command, verify outcomes
  </action>
  <verify>
    1. `bun test tests/integration/commands/` -- all 6 test files pass
    2. `bun test` -- all tests (unit + integration) pass together
    3. No test depends on real filesystem, network, or process.cwd()
    4. Each command test verifies at least 3 scenarios (happy path, error path, edge case)
  </verify>
  <done>
    All 6 CLI commands have integration tests covering happy paths, error paths, and interactive flows. All tests pass with fully mocked I/O (filesystem, fetch, prompts, console). Commands are tested via their exported functions with projectRoot injection.
  </done>
</task>

<task type="auto">
  <name>Task 2: Coverage validation and threshold enforcement</name>
  <files>
    tests/integration/commands/init.test.ts
    tests/integration/commands/add.test.ts
    tests/integration/commands/sync.test.ts
    tests/integration/commands/status.test.ts
    tests/integration/commands/clean.test.ts
    tests/integration/commands/update.test.ts
  </files>
  <action>
    Run the full test suite with coverage and validate that per-module thresholds are met. If any module is below 80%, add targeted tests to close gaps.

    **Step 1: Run coverage**
    ```
    bun test --coverage
    ```
    Review the text output. Check which modules are below 80% lines or functions.

    **Step 2: Run per-module check**
    ```
    bun run scripts/check-coverage.ts
    ```
    This will report PASS/FAIL per module.

    **Step 3: Close coverage gaps**
    For any module below 80%, analyze what's uncovered by examining the lcov.info file or the text coverage output. Add targeted test cases to the appropriate unit test files (from Plans 02/03) or integration test files (from Task 1 above).

    Common coverage gaps to anticipate:
    - Error branches in config.ts (invalid JSON, missing directories)
    - Redirect handling in context7-client.ts (library_redirected path)
    - Edge cases in extractContext7Content (unusual response formats)
    - fs-utils.ts deeply nested directory operations
    - Commands with multiple interactive paths

    If a gap is in a command file (src/commands/*), add an integration test. If in a lib file (src/lib/*), add a unit test.

    **Step 4: Verify the full pipeline**
    ```
    bun test --coverage && bun run scripts/check-coverage.ts
    ```
    This is the exact command that will run in CI. It must exit 0.

    Note: Branch coverage is NOT enforced (Bun doesn't support it -- GitHub #7100). Only line and function coverage are checked, per research findings. This is documented in the research and accepted as a known limitation.
  </action>
  <verify>
    1. `bun test --coverage` shows all tests passing
    2. `bun run scripts/check-coverage.ts` shows PASS for all modules (config, templates, index-parser, fs-utils, context7, context7-client, mcp-client, index-utils)
    3. `bun test --coverage && bun run scripts/check-coverage.ts` exits with code 0
    4. Coverage text output shows 80%+ lines for each src/lib/ module
    5. coverage/lcov.info file exists (for Phase 2 CI tooling)
  </verify>
  <done>
    The complete test suite passes with 80%+ per-module coverage (lines and functions) for all src/lib/ modules. The per-module coverage script validates thresholds and exits 0. The lcov.info output is generated for future CI integration. The full test pipeline (`bun test --coverage && bun run scripts/check-coverage.ts`) succeeds end-to-end.
  </done>
</task>

</tasks>

<verification>
1. `bun test` -- ALL tests pass (unit + integration + coverage-loader)
2. `bun test --coverage && bun run scripts/check-coverage.ts` -- exits 0
3. `bun run typecheck` -- still passes
4. `bun run build` -- still passes
5. Each module in src/lib/ shows 80%+ line coverage
6. coverage/lcov.info exists and contains data for all modules
7. No test depends on network, filesystem, or external state
</verification>

<success_criteria>
- Running `bun test` executes unit tests for config, templates, index-parser, fs-utils, context7-client, mcp-client, context7, index-utils AND integration tests for init, add, sync, status, clean, update -- ALL passing
- Per-module coverage script confirms 80%+ on each src/lib/ module
- External I/O (filesystem, Context7 HTTP, MCP CLI, prompts) is mocked in every test
- coverage/lcov.info generated for CI tooling
- The complete test pipeline exits 0
</success_criteria>

<output>
After completion, create `.planning/phases/01-testing-infrastructure/01-04-SUMMARY.md`
</output>
